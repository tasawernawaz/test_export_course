0
00:00:00,000 --> 00:00:02,820


1
00:00:02,820 --> 00:00:05,520
Let's look at some special probability distributions.

2
00:00:05,520 --> 00:00:07,920
Those that are very important, they come up frequently,

3
00:00:07,920 --> 00:00:10,470
and they're very well understood.

4
00:00:10,470 --> 00:00:12,900
The simplest one is the Bernoulli random variable,

5
00:00:12,900 --> 00:00:15,450
and there, we're flipping a biased coin.

6
00:00:15,450 --> 00:00:18,480
Biased, in this case, means that the probability

7
00:00:18,480 --> 00:00:21,020
of heads or tails is not necessarily one half.

8
00:00:21,020 --> 00:00:23,910


9
00:00:23,910 --> 00:00:27,560
The name of the random variable is going to be x superscript b.

10
00:00:27,560 --> 00:00:32,580
xb equals 1 if the outcome is heads, 0 if it's tails.

11
00:00:32,580 --> 00:00:34,320
So p is going to be that probability.

12
00:00:34,320 --> 00:00:37,870
P is a real number between 0 and 1.

13
00:00:37,870 --> 00:00:42,900
And the probability that xb equals 1 is equal to p.

14
00:00:42,900 --> 00:00:46,320
So therefore, the probability that xb equals 0,

15
00:00:46,320 --> 00:00:49,310
is equal to 1 minus p.

16
00:00:49,310 --> 00:00:54,300
So xb is called a Bernoulli random variable.

17
00:00:54,300 --> 00:00:56,150
The next one, which is based on those,

18
00:00:56,150 --> 00:00:59,060
is the binomial random variable.

19
00:00:59,060 --> 00:01:02,930
Let's suppose we flip n coins, and we

20
00:01:02,930 --> 00:01:06,710
count the number of heads that we get with those n coins.

21
00:01:06,710 --> 00:01:08,540
There's the binomial random variable.

22
00:01:08,540 --> 00:01:11,720
So it's the sum of n independent random variables

23
00:01:11,720 --> 00:01:13,960
with the same parameter p.

24
00:01:13,960 --> 00:01:17,630
It's called x lowercase b.

25
00:01:17,630 --> 00:01:21,470
Here's the formula defining the binomial random variable,

26
00:01:21,470 --> 00:01:23,600
and here's the probability distribution

27
00:01:23,600 --> 00:01:25,880
of the binomial random variable.

28
00:01:25,880 --> 00:01:28,700
Don't worry about memorizing that formula.

29
00:01:28,700 --> 00:01:31,810


30
00:01:31,810 --> 00:01:35,640
Here's a graph of the binomial probability distribution.

31
00:01:35,640 --> 00:01:38,010
The vertical axis is the probability.

32
00:01:38,010 --> 00:01:40,560
The horizontal axis is the number of heads

33
00:01:40,560 --> 00:01:43,310
that we get from the coins that we flip.

34
00:01:43,310 --> 00:01:45,750
For this graph, we're flipping 100 coins,

35
00:01:45,750 --> 00:01:48,270
or we're flipping one coin 100 times,

36
00:01:48,270 --> 00:01:52,770
and the probability of getting heads is 0.4.

37
00:01:52,770 --> 00:01:55,830
This has a very important distinctive shape.

38
00:01:55,830 --> 00:01:58,410
It looks like the Gaussian, or normal random variable,

39
00:01:58,410 --> 00:02:00,660
which we're going to talk about in great detail later.

40
00:02:00,660 --> 00:02:03,670


41
00:02:03,670 --> 00:02:05,770
This represents the number of times

42
00:02:05,770 --> 00:02:08,575
we have to flip a coin until we get the first heads.

43
00:02:08,575 --> 00:02:13,300


44
00:02:13,300 --> 00:02:15,670
The coin is a biased coin with probability p

45
00:02:15,670 --> 00:02:18,860
of getting a heads.

46
00:02:18,860 --> 00:02:22,130
So here's a schematic of what this looks like.

47
00:02:22,130 --> 00:02:24,384
The first time we flip a coin, we get a tails.

48
00:02:24,384 --> 00:02:26,300
The second time we flip a coin, we get a tails

49
00:02:26,300 --> 00:02:29,240
and so forth, until finally, the k-th time we flip the coin

50
00:02:29,240 --> 00:02:31,340
and we get heads.

51
00:02:31,340 --> 00:02:32,780
The value of this random variable

52
00:02:32,780 --> 00:02:37,310
is k in this outcome of the experiment.

53
00:02:37,310 --> 00:02:39,860
Formally, we can define the geometric random variable

54
00:02:39,860 --> 00:02:40,735
this way--

55
00:02:40,735 --> 00:02:48,350
xg equals k if x1 b equals 0, x2 b equals 0, and so forth, up

56
00:02:48,350 --> 00:02:55,700
to xb k minus 1, and then xb k equals 1, where the xb's

57
00:02:55,700 --> 00:02:59,210
are Bernoulli random variables.

58
00:02:59,210 --> 00:03:02,180
OK, let's look at how to calculate the probability

59
00:03:02,180 --> 00:03:03,800
that xg equals k.

60
00:03:03,800 --> 00:03:07,460
In other words, the probability distribution of xg.

61
00:03:07,460 --> 00:03:11,540
We observe that the probability that xg equals 1 equals p.

62
00:03:11,540 --> 00:03:14,810
That's because xg equals 1 means that we

63
00:03:14,810 --> 00:03:16,730
got a heads on the first flip.

64
00:03:16,730 --> 00:03:20,360
So the probability of getting a heads on that flip is p.

65
00:03:20,360 --> 00:03:22,730
If we don't get a heads on the first flip,

66
00:03:22,730 --> 00:03:25,490
that probability is 1 minus p.

67
00:03:25,490 --> 00:03:30,050
The event that we didn't get the first head on the first flip,

68
00:03:30,050 --> 00:03:33,980
means that we got the first head on some later flip.

69
00:03:33,980 --> 00:03:36,590
In other words, xg is greater than 1.

70
00:03:36,590 --> 00:03:38,780
So the probability that xg is greater than 1

71
00:03:38,780 --> 00:03:40,050
is equal to last 1 minus p.

72
00:03:40,050 --> 00:03:43,020


73
00:03:43,020 --> 00:03:44,700
Also, we're going to need to observe

74
00:03:44,700 --> 00:03:47,310
that the event that xg is greater

75
00:03:47,310 --> 00:03:51,240
than k is a subset of the event, xg greater than k minus 1.

76
00:03:51,240 --> 00:03:54,060


77
00:03:54,060 --> 00:03:56,940
OK, we're going to make use of conditional probability

78
00:03:56,940 --> 00:04:01,080
to calculate the probability that xj is greater than k.

79
00:04:01,080 --> 00:04:02,700
The probability that xj is greater

80
00:04:02,700 --> 00:04:04,275
than k is equal to the probability

81
00:04:04,275 --> 00:04:08,850
that xj is greater than k given that xj is greater than k

82
00:04:08,850 --> 00:04:12,420
minus 1, times the probability of xg is greater than k

83
00:04:12,420 --> 00:04:15,010
minus 1.

84
00:04:15,010 --> 00:04:16,800
And the probability that xg is greater

85
00:04:16,800 --> 00:04:20,130
than k, given that xg is greater than k minus 1,

86
00:04:20,130 --> 00:04:23,700
is simply the probability of the event

87
00:04:23,700 --> 00:04:27,370
that we do not get a head on the k

88
00:04:27,370 --> 00:04:31,590
flip, and given that we haven't before that either.

89
00:04:31,590 --> 00:04:34,320
So that probability is just 1 minus p.

90
00:04:34,320 --> 00:04:36,330
So the probability that xg is greater than k,

91
00:04:36,330 --> 00:04:38,190
is one minus p times the probability

92
00:04:38,190 --> 00:04:42,405
that xj is greater than k minus 1.

93
00:04:42,405 --> 00:04:45,780
To look at the conditional probability more formally,

94
00:04:45,780 --> 00:04:48,120
the probability that xg is greater than k,

95
00:04:48,120 --> 00:04:51,700
given that xg is greater than k minus 1.

96
00:04:51,700 --> 00:04:54,870
This is because the probability that we get a heads

97
00:04:54,870 --> 00:04:58,950
after the k flip, given that the head comes first

98
00:04:58,950 --> 00:05:01,950
after the k minus first flip, is just

99
00:05:01,950 --> 00:05:04,110
the probability of the event that we

100
00:05:04,110 --> 00:05:07,290
get all tails until the k flip, given

101
00:05:07,290 --> 00:05:10,440
that we got all tails until the k minus first flip.

102
00:05:10,440 --> 00:05:13,480


103
00:05:13,480 --> 00:05:17,020
And that's just 1 minus p.

104
00:05:17,020 --> 00:05:22,030
So the probability that xg is greater than 1, is 1 minus p.

105
00:05:22,030 --> 00:05:24,310
The probability that xg is greater than 2,

106
00:05:24,310 --> 00:05:25,645
is 1 minus p squared.

107
00:05:25,645 --> 00:05:32,230


108
00:05:32,230 --> 00:05:34,030
The probability that xg is greater than 2

109
00:05:34,030 --> 00:05:36,550
is 1 minus p squared, just making

110
00:05:36,550 --> 00:05:40,330
use of that conditional probability formula once.

111
00:05:40,330 --> 00:05:42,850
We do it again, and the probability

112
00:05:42,850 --> 00:05:45,460
that xg is greater than 3 is greater than 1 minus 3

113
00:05:45,460 --> 00:05:46,330
cubed, and so forth.

114
00:05:46,330 --> 00:05:49,720


115
00:05:49,720 --> 00:05:53,470
So the probability that xg is greater than k minus 1,

116
00:05:53,470 --> 00:05:55,420
is just 1 minus p to the k minus 1.

117
00:05:55,420 --> 00:05:59,120


118
00:05:59,120 --> 00:06:02,790
The probability that xk equals 1,

119
00:06:02,790 --> 00:06:07,140
is the probability that xg is greater than k minus 1,

120
00:06:07,140 --> 00:06:09,960
and xb k equals 1.

121
00:06:09,960 --> 00:06:16,030


122
00:06:16,030 --> 00:06:19,770
In other words, we got the first head on the k flip,

123
00:06:19,770 --> 00:06:24,740
and that is simply 1 minus p to the k minus 1 times p.

124
00:06:24,740 --> 00:06:28,940


125
00:06:28,940 --> 00:06:32,370
Here's a graph of the geometric probability distribution.

126
00:06:32,370 --> 00:06:33,700
So we're flipping a coin.

127
00:06:33,700 --> 00:06:36,830
The probability of getting a heads is 0.1.

128
00:06:36,830 --> 00:06:39,590
the vertical bars represent the probability

129
00:06:39,590 --> 00:06:43,890
of getting the first head at the n-th coin flip.

130
00:06:43,890 --> 00:06:47,600


131
00:06:47,600 --> 00:06:49,285
Another one is a Poisson distribution.

132
00:06:49,285 --> 00:06:51,160
We're not going to talk about that now that's

133
00:06:51,160 --> 00:06:53,220
going to come up later.

